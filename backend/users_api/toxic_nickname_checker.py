import csv
from pathlib import Path
from typing import List, Tuple, Dict
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import os
import sys
import django

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'backend.config.settings')
django.setup()

from users_api.models import NicknameDataset

# --- –ö–û–ù–°–¢–ê–ù–¢–´ –ò –†–ê–°–®–ò–†–ï–ù–ù–´–ï –°–õ–û–í–ê–†–ò (V10 - –£–°–¢–†–ê–ù–ï–ù–ò–ï FN) ---

CSV_FILE_NAME = 'toxic_nicks.csv' 
TOXIC_DATASET_RU: List[str] = []
TOXIC_DATASET_OTHER: List[str] = []

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∏ —Ç–æ—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –≥–æ–º–æ–≥–ª–∏—Ñ–æ–≤
HOMOGLYPH_MAP: Dict[str, str] = {
    # –ß–∏—Å–ª–∞ –∏ —Å–∏–º–≤–æ–ª—ã
    '4': '–∞', '@': '–∞', '6': '–±', '3': '–µ', '1': '–∏', '0': '–æ', '2': '–∑', '5': '—Å',
    
    # –õ–∞—Ç–∏–Ω–∏—Ü–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ —Å—Ç–∞—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ leet speak
    'p': '–ø',  # FIX: p (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> –ø (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞) [–î–ª—è –ü–∏–¥–æ—Ä]
    'u': '—É',  # FIX: u (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —É (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞) [–î–ª—è –°—É–∫–∞]
    'r': '—Ä',  # r (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —Ä (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'y': '—É',  # y (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —É (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'h': '–Ω',  # h (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> –Ω (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'c': '—Å',  # c (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —Å (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'x': '—Ö',  # x (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —Ö (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'b': '–±',  # b (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> –± (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞) [–î–ª—è –ï–±–ª–∞–Ω, –¢–≤–∞—Äb]
    'l': '–ª',  # l (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> –ª (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'd': '–¥',  # d (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> –¥ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    't': '—Ç',  # t (–ª–∞—Ç–∏–Ω–∏—Ü–∞) -> —Ç (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    'f': '—Ñ',
    'v': '–≤',
    'w': '—à',
    'k': '–∫',
    'j': '–∏', 
    'q': '–∫',
}

def normalize_homoglyphs(text: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç –≥–æ–º–æ–≥–ª–∏—Ñ—ã –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã."""
    normalized = text.lower()
    for lat, cyr in HOMOGLYPH_MAP.items():
        normalized = normalized.replace(lat, cyr)
    return normalized

# --- –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–• (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---

def load_toxic_data_from_db() -> Tuple[List[str], List[str]]:
    ru_list = []
    other_list = []
    
    try:
        toxic_nicknames = NicknameDataset.objects.filter(is_toxic=True)
        
        for entry in toxic_nicknames:
            word = entry.nickname.lower()
            lang = entry.language_type
            
            # –û—á–∏—Å—Ç–∫–∞ —Å–ª–æ–≤ –≤ –±–∞–∑–µ –æ—Ç –∑–∞–≤–µ–¥–æ–º—ã—Ö –º–∞—Å–∫–∏—Ä–æ–≤–æ–∫
            cleaned_word = (word.replace('@', '–∞').replace('0', '–æ').replace('3', '–µ').replace('1', '–∏'))

            if lang == 'ru':
                ru_list.append(cleaned_word)
            else:
                other_list.append(word)

        print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(ru_list)} —Ä—É—Å—Å–∫–∏—Ö –∏ {len(other_list)} –Ω–µ—Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –ë–î.")
        return ru_list, other_list
    except Exception as e:
        print(f"\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∏–∑ –ë–î: {e}")
        return [], []

# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–†–û–í–ï–†–ö–ò (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---

def check_nickname_toxicity(nickname: str) -> bool:
    if not nickname:
        return False
        
    normalized_nickname = nickname.lower()
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ (EN, CN)
    for toxic_word in TOXIC_DATASET_OTHER:
        if toxic_word in normalized_nickname: 
            return True
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –≥–æ–º–æ–≥–ª–∏—Ñ–æ–≤
    homoglyph_normalized_nickname = normalize_homoglyphs(normalized_nickname)
    for toxic_word in TOXIC_DATASET_RU:
        if toxic_word in homoglyph_normalized_nickname:
            return True 
            
    return False 

# --- –§–£–ù–ö–¶–ò–Ø –û–¶–ï–ù–ö–ò –ò –ì–†–ê–§–ò–ö–û–í (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---

def evaluate_and_plot(eval_data: List[Tuple[str, bool]]):
    actual_labels = [item[1] for item in eval_data]
    predicted_labels = []

    print("\n--- –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ ---")
    for nickname, actual_is_toxic in eval_data:
        predicted_is_toxic = check_nickname_toxicity(nickname)
        predicted_labels.append(predicted_is_toxic)
        
        status = "‚úÖ –í–µ—Ä–Ω–æ" if predicted_is_toxic == actual_is_toxic else "‚ùå –û—à–∏–±–∫–∞"
        if predicted_is_toxic == True and actual_is_toxic == True: match_type = "TP"
        elif predicted_is_toxic == False and actual_is_toxic == False: match_type = "TN" 
        elif predicted_is_toxic == True and actual_is_toxic == False: match_type = "FP"
        elif predicted_is_toxic == False and actual_is_toxic == True: match_type = "FN"

        print(f"[{status} {match_type}] '{nickname}' -> –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {predicted_is_toxic}, –û–∂–∏–¥–∞–ª–æ—Å—å: {actual_is_toxic}")

    # –ù–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç TP, TN, FP, FN
    TP = sum(p and a for p, a in zip(predicted_labels, actual_labels))
    TN = sum(not p and not a for p, a in zip(predicted_labels, actual_labels))
    FP = sum(p and not a for p, a in zip(predicted_labels, actual_labels))
    FN = sum(not p and a for p, a in zip(predicted_labels, actual_labels))
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
    cm = np.array([[TP, FP], [FN, TN]]) 
    
    accuracy = (TP + TN) / len(eval_data)
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    
    print("\n--- –ú–ï–¢–†–ò–ö–ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò ---")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f}")
    print(f"–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ—Å—Ç—å (Precision): {precision:.4f}")
    print(f"–ü–æ–ª–Ω–æ—Ç–∞ (Recall): {recall:.4f}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["–¢–æ–∫—Å–∏—á–Ω—ã–π", "–ù–µ—Ç–æ–∫—Å–∏—á–Ω—ã–π"])
    fig, ax = plt.subplots(figsize=(7, 7))
    disp.plot(cmap=plt.cm.Blues, ax=ax)
    ax.set_title("–ú–∞—Ç—Ä–∏—Ü–∞ –û—à–∏–±–æ–∫ (V10)")
    
    plt.show()

# --- –¢–ï–°–¢–û–í–´–ï –î–ê–ù–ù–´–ï (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï) ---
EVAL_DATA: List[Tuple[str, bool]] = [
    ("T–≤–∞—Äb_–Ω–∞_x—Ée", True),      
    ("3blan4ik", True),          
    ("P1d0r4s666", True),        
    ("cuka_cyka", True),         
    ("–ü–∏–¥–æ—Ä–∞—Å—Å–°", True),
    ("–£–ë–õ–Æ–î–û–ö11", True),
    ("‰Ω†Â¶àÊòØÁÖûÁ¨î", True), 
    ("Caravan", False),
    ("NormalPlayer", False),
    ("GamerPro2024", False),
    ("MyNickname", False),
    ("HomerSimpson", False),
    ("NotToxicNick", False), 
    ("qwertydick", True),
    ("—Ö—É–π", True),
    ("fuck", True),
]

# --- –§–£–ù–ö–¶–ò–Ø –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---

def interactive_test(current_data: List[Tuple[str, bool]]):
    print("\n" + "="*50)
    print("ü§ñ –†–ï–ñ–ò–ú –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("–í–≤–µ–¥–∏—Ç–µ 'end' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞.")
    print("="*50)
    
    while True:
        nickname = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–∏–∫–Ω–µ–π–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: ")
        if nickname.lower() == 'end':
            break
        
        try:
            expected_label = input("–≠—Ç–æ —Ç–æ–∫—Å–∏—á–Ω—ã–π –Ω–∏–∫? (True/False): ")
            if expected_label.lower() not in ['true', 'false']:
                print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Ç–æ–ª—å–∫–æ 'True' –∏–ª–∏ 'False'.")
                continue
                
            expected_label_bool = expected_label.lower() == 'true'
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞
            predicted_is_toxic = check_nickname_toxicity(nickname)
            status = "‚úÖ –í–ï–†–ù–û" if predicted_is_toxic == expected_label_bool else "‚ùå –û–®–ò–ë–ö–ê"
            
            print(f"\n--- –†–ï–ó–£–õ–¨–¢–ê–¢ ---")
            print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –Ω–∏–∫: {normalize_homoglyphs(nickname)}")
            print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {'–¢–û–ö–°–ò–ß–ù–´–ô' if predicted_is_toxic else '–ß–ò–°–¢–´–ô'}")
            print(f"–í–µ—Ä–¥–∏–∫—Ç: {status}. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ {predicted_is_toxic}, –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_label_bool}.")
            print("-----------------")

            current_data.append((nickname, expected_label_bool))

        except Exception as e:
            print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–≤–æ–¥–∞: {e}")
            continue

# --- –ì–õ–ê–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê ---
if __name__ == "__main__":
    
    print(f"--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î ---")
    TOXIC_DATASET_RU, TOXIC_DATASET_OTHER = load_toxic_data_from_db()
    
    if not TOXIC_DATASET_RU and not TOXIC_DATASET_OTHER:
        print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–∞–Ω–Ω—ã—Ö.")
    else:
        # 1. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –¢–ï–°–¢–´
        evaluate_and_plot(EVAL_DATA)

        # 2. –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú
        interactive_test(EVAL_DATA)
        
        # 3. –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê
        if len(EVAL_DATA) > 16: 
             print("\n" + "="*50)
             print("üìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê (—Å —É—á–µ—Ç–æ–º –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –≤–∞–º–∏ –Ω–∏–∫–æ–≤)")
             evaluate_and_plot(EVAL_DATA)
